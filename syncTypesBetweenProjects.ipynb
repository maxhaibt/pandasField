{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 494\u001b[0m\n\u001b[0;32m    488\u001b[0m targetdb \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39muruk\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    489\u001b[0m \u001b[39m#shapesdblist = [db for db in shapesdblist if db in selectlist ]\u001b[39;00m\n\u001b[0;32m    490\u001b[0m \u001b[39m## for testing only one db ##\u001b[39;00m\n\u001b[0;32m    491\u001b[0m \u001b[39m#shapesdblist = [db for db in shapesdblist if db in selectlist ]\u001b[39;00m\n\u001b[0;32m    492\u001b[0m \u001b[39m#print(shapesdblist)\u001b[39;00m\n\u001b[1;32m--> 494\u001b[0m targetdbrows \u001b[39m=\u001b[39m getAllDocsv2(db_name \u001b[39m=\u001b[39;49m targetdb)\n\u001b[0;32m    495\u001b[0m allsourcedbrows \u001b[39m=\u001b[39m []\n\u001b[0;32m    497\u001b[0m \u001b[39m#print(targetdbdocs['rows'])\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 40\u001b[0m, in \u001b[0;36mgetAllDocsv2\u001b[1;34m(db_name)\u001b[0m\n\u001b[0;32m     38\u001b[0m pouchDB_url_all \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mdb_url\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mdb_name\u001b[39m}\u001b[39;00m\u001b[39m/_all_docs\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     39\u001b[0m response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mpost(pouchDB_url_all, auth\u001b[39m=\u001b[39mauth)\n\u001b[1;32m---> 40\u001b[0m result \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39;49mloads(response\u001b[39m.\u001b[39;49mtext)\n\u001b[0;32m     41\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\mhaibt\\.conda\\envs\\accessidaifield\\lib\\json\\__init__.py:357\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    352\u001b[0m     \u001b[39mdel\u001b[39;00m kw[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    354\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    355\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    356\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[1;32m--> 357\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[0;32m    358\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32mc:\\Users\\mhaibt\\.conda\\envs\\accessidaifield\\lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, s, _w\u001b[39m=\u001b[39mWHITESPACE\u001b[39m.\u001b[39mmatch):\n\u001b[0;32m    333\u001b[0m     \u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[39m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[0;32m    338\u001b[0m     end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m     \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n",
      "File \u001b[1;32mc:\\Users\\mhaibt\\.conda\\envs\\accessidaifield\\lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    353\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscan_once(s, idx)\n\u001b[0;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExpecting value\u001b[39m\u001b[39m\"\u001b[39m, s, err\u001b[39m.\u001b[39mvalue) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    356\u001b[0m \u001b[39mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "import operator\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "from math import isnan\n",
    "import itertools\n",
    "from decimal import Decimal  \n",
    "import lib.pdfield as pdfield\n",
    "#from decimal import Decimal  \n",
    "\n",
    "FIG_SIZE = [20,30]\n",
    "\n",
    "def divide_chunks(l, n):\n",
    "    # looping till length l\n",
    "    for i in range(0, len(l), n): \n",
    "        yield l[i:i + n]\n",
    "def bulkSaveChanges(DOC, pouchDB_url_bulk, auth ):\n",
    "    chunks = list(divide_chunks(DOC['docs'], 200))\n",
    "    for chunk in chunks:\n",
    "        #print(json.dumps(chunk, indent=4, sort_keys=True))\n",
    "        chunkhull = {'docs':[]}\n",
    "        chunkhull['docs'] = chunk\n",
    "        answer = requests.post(pouchDB_url_bulk , auth=auth, json=chunkhull)\n",
    "        print(answer)\n",
    "    return print('Documents uploaded')\n",
    "\n",
    "def getListOfDBs():\n",
    "    response = requests.get(pouchDB_url_alldbs, auth=auth)\n",
    "    result = json.loads(response.text)\n",
    "    return result\n",
    "\n",
    "def getAllDocsv2(db_name):\n",
    "    pouchDB_url_all = f'{db_url}/{db_name}/_all_docs'\n",
    "    response = requests.post(pouchDB_url_all, auth=auth)\n",
    "    result = json.loads(response.text)\n",
    "    return result\n",
    "\n",
    "\n",
    "def getAllDocs(db_name):\n",
    "    pouchDB_url_find = f'{db_url}/{db_name}/_find'\n",
    "    querydict={'selector':{}}\n",
    "    querydict['selector']['_id'] = {'$gt': None}\n",
    "    response = requests.post(pouchDB_url_find, auth=auth, json=querydict)\n",
    "    result = json.loads(response.text)\n",
    "    return result\n",
    "\n",
    "def getAllDrawings(db_name):\n",
    "    pouchDB_url_find = f'{db_url}/{db_name}/_find'\n",
    "    querydict={'selector':{}}\n",
    "    querydict['selector']['resource.type'] = {'$eq': 'Drawing'}\n",
    "    response = requests.post(pouchDB_url_find, auth=auth, json=querydict)\n",
    "    result = json.loads(response.text)\n",
    "    return result\n",
    "\n",
    "def getDocsInList(db_name, list):\n",
    "    pouchDB_url_find = f'{db_url}/{db_name}/_find'\n",
    "    querydict={'selector':{}}\n",
    "    querydict['selector']['_id'] = {'$in': list}\n",
    "    response = requests.post(pouchDB_url_find, auth=auth, json=querydict)\n",
    "    result = json.loads(response.text)\n",
    "    return result\n",
    "\n",
    "def idOfIdentifier(identifier, auth, pouchDB_url_find):\n",
    "    queryByIdentifier={'selector':{}}\n",
    "    queryByIdentifier['selector']['resource.identifier'] = {'$eq':str(identifier)}\n",
    "    response = requests.post(pouchDB_url_find, auth=auth, json=queryByIdentifier)\n",
    "    result = json.loads(response.text)\n",
    "    #print (result)\n",
    "    return result['docs'][0]['resource']['id']\n",
    "\n",
    "def identifierOfId(id, auth, pouchDB_url_find):\n",
    "    queryByIdentifier={'selector':{}}\n",
    "    queryByIdentifier['selector']['resource.id'] = {'$eq':str(id)}\n",
    "    response = requests.post(pouchDB_url_find, auth=auth, json=queryByIdentifier)\n",
    "    result = json.loads(response.text)\n",
    "    #print (result)\n",
    "    return result['docs'][0]['resource']['identifier']\n",
    "\n",
    "def getDocsRecordedInIdentifier(identifier, auth, pouchDB_url_find):\n",
    "    querydict={'selector':{}}\n",
    "    querydict['selector']['resource.relations.isRecordedIn'] = {'$elemMatch': str(idOfIdentifier(str(identifier), auth=auth, pouchDB_url_find=pouchDB_url_find))}\n",
    "    response = requests.post(pouchDB_url_find, auth=auth, json=querydict)\n",
    "    result = json.loads(response.text)\n",
    "    return result\n",
    "def getDocsNotRecordedInIdentifier(identifier, auth, pouchDB_url_find):\n",
    "    querydict={'selector':{'$not':{'resource.relations.isRecordedIn':{}}}}\n",
    "    selector1 = {'$in': [str(idOfIdentifier(str(identifier), auth=auth, pouchDB_url_find=pouchDB_url_find)), str(idOfIdentifier('StadtSurvey', auth=auth, pouchDB_url_find=pouchDB_url_find)) ]}\n",
    "    querydict['selector']['$not']['resource.relations.isRecordedIn'] = selector1\n",
    "    response = requests.post(pouchDB_url_find, auth=auth, json=querydict)\n",
    "    result = json.loads(response.text)\n",
    "    print (querydict)\n",
    "    return result\n",
    "def addModifiedEntry(doc):\n",
    "    now = datetime.now()\n",
    "    entry = {}\n",
    "    entry['user'] = 'Script mhaibt'\n",
    "    daytoSec = now.strftime('%Y-%m-%dT%H:%M:%S')\n",
    "    sec = \"{:.3f}\".format(Decimal(now.strftime('.%f')))\n",
    "    entry['date'] = daytoSec + str(sec)[1:] + 'Z'\n",
    "    #print(doc)\n",
    "    doc['modified'].append(entry)\n",
    "    return doc\n",
    "\n",
    "def saveChanges(doc, pouchDB_url_put, auth ):\n",
    "    requests.put(pouchDB_url_put + doc['_id'], auth=auth, json=doc)\n",
    "\n",
    "\n",
    "\n",
    "def statOfRessouceTypes(result):\n",
    "    dataset = sorted(result['docs'], key= lambda x: x['resource']['type'])\n",
    "    grouped = itertools.groupby(dataset,key = lambda x: x['resource']['type'])\n",
    "    for k, v in grouped:\n",
    "        print (k, len(list(v)))\n",
    "    print('That was statOfRessouceTypes')\n",
    "    return grouped\n",
    "\n",
    "def statOfRecordedIn(result):\n",
    "    dataset = sorted(result, key= lambda x: x['resource']['relations']['isRecordedIn'][0])\n",
    "    grouped = itertools.groupby(dataset,key = lambda x: x['resource']['relations']['isRecordedIn'][0])\n",
    "    for k, v in grouped:\n",
    "        print (str(identifierOfId(str(k), auth=auth, pouchDB_url_find=pouchDB_url_find)), len(list(v)))\n",
    "    print('That was statOfRecordedIn')\n",
    "    return grouped\n",
    "def statOfLiesWithin(result):\n",
    "    dataset = sorted(result, key= lambda x: x['resource']['relations']['liesWithin'][0])\n",
    "    grouped = itertools.groupby(dataset,key = lambda x: x['resource']['relations']['liesWithin'][0])\n",
    "    for k, v in grouped:\n",
    "        print (str(identifierOfId(str(k), auth=auth, pouchDB_url_find=pouchDB_url_find)), len(list(v)))\n",
    "    print('That was statOfLiesWithin')\n",
    "    return grouped\n",
    "\n",
    "def filterOfResourceTypes(listOfNotIncludedTypes, result):\n",
    "    filteredResources = [obj for obj in result['docs'] if not obj['resource']['type'] in listOfNotIncludedTypes ]\n",
    "    return filteredResources\n",
    "def selectOfResourceTypes(listOfIncludedTypes, result):\n",
    "    selectedResources = [obj for obj in result if 'type' in obj['resource'].keys() ]\n",
    "    selectedResources = [obj for obj in selectedResources if obj['resource']['type'] in listOfIncludedTypes ]\n",
    "    return selectedResources\n",
    "def replaceValue (series, key, value, newvalue):\n",
    "    #print(key, series[key])\n",
    "    if type(series[key])==list:\n",
    "        if value in series[key]:\n",
    "            \n",
    "            series[key].remove(value)\n",
    "            series[key].append(newvalue)\n",
    "            series[key] = list(set(series[key]))\n",
    "            print(key, series[key])\n",
    "    return series\n",
    "\n",
    "def combineColumns (series, columnlist, newColumn):\n",
    "    for column in columnlist:\n",
    "        if type(series[column])==list:\n",
    "            #print(column, ' is a List')\n",
    "            series[newColumn]=series[column]\n",
    "        if type(series[column])==str:\n",
    "        #print(column, ' is a List')\n",
    "            series[newColumn]=series[column]\n",
    "        else:\n",
    "            if math.isnan(series[column]):\n",
    "                #print(column, ' is NaN')\n",
    "                continue\n",
    "                #series.drop(column)\n",
    "    columns = [i for i in columnlist if not i == newColumn]\n",
    "    #print(columns)\n",
    "    series = series.drop(columns)\n",
    "    return series\n",
    "\n",
    "def strColumnToList (series, column):\n",
    "    if type(series[column]) == str:\n",
    "        valueaslist = []\n",
    "        valueaslist.append(series[column])\n",
    "        series[column] = valueaslist\n",
    "    return series\n",
    "\n",
    "\n",
    "def findEliminateNone(row):\n",
    "    #test = json.loads(row['dimensionDiameter'][0])\n",
    "    if type(row['dimensionDiameter']) == list:\n",
    "        #if 'dimensionDiameter' in row['dimensionDiameter'][0]:\n",
    "        #print(json.loads(row['dimensionDiameter'][0]))\n",
    "        #if 'inputRangeEndValue' in row['dimensionDiameter'][0]:\n",
    "        if 'inputValue' in row['dimensionDiameter'][0] and row['dimensionDiameter'][0]['inputValue'] == None:\n",
    "            #if row['dimensionDiameter'][0]['isRange']== False:\n",
    "            print (row['identifier'])\n",
    "                #print (row['modified'])\n",
    "            print(row['dimensionDiameter'])\n",
    "            del row['dimensionDiameter'][0]\n",
    "                #print(row['dimensionDiameter'])\n",
    "    #elif math.isnan(row['dimensionDiameter']):\n",
    "        #print('Nonono')\n",
    "\n",
    "    return row\n",
    "\n",
    "def correctDiameter(row):\n",
    "    #test = json.loads(row['dimensionDiameter'][0])\n",
    "    if type(row['dimensionDiameter']) == list:\n",
    "        #if 'dimensionDiameter' in row['dimensionDiameter'][0]:\n",
    "        #print(row['dimensionDiameter'][0])\n",
    "        #if not math.isnan(row['dimensionDiameter']):\n",
    "            #row['dimensionDiameter'] = np.nan\n",
    "            #print(row['identifier'], row['dimensionDiameter'] )\n",
    "            #row = row.drop('dimensionDiameter')\n",
    "            #if row['dimensionDiameter']:\n",
    "                #print(row['dimensionDiameter'] )\n",
    "        if 'isRange' in row['dimensionDiameter'][0] and row['dimensionDiameter'][0]['isRange']== False and 'rangeMin' in row['dimensionDiameter'][0]:\n",
    "            print(row['dimensionDiameter'][0]['inputValue'], row['dimensionDiameter'][0]['rangeMin'], row['dimensionDiameter'][0]['rangeMax'])\n",
    "            del row['dimensionDiameter'][0]['rangeMin']\n",
    "            del row['dimensionDiameter'][0]['rangeMax']\n",
    "            if 'inputRangeEndValue' in row['dimensionDiameter'][0]:\n",
    "                print(row['dimensionDiameter'][0])\n",
    "                del row['dimensionDiameter'][0]['inputRangeEndValue']\n",
    "                print(row['dimensionDiameter'][0])\n",
    "                #row['dimensionDiameter'][0].pop('inputRangeEndValue')\n",
    "            #del row['dimensionDiameter'][0]['inputRangeEndValue']\n",
    "            #row['dimensionDiameter'][0]['isRange']\n",
    "        #if 'inputValue' in row['dimensionDiameter'][0] and row['dimensionDiameter'][0]['inputValue'] == None:\n",
    "            #if row['dimensionDiameter'][0]['isRange']== False:\n",
    "            #print (row['identifier'])\n",
    "                #print (row['modified'])\n",
    "            #print(row['dimensionDiameter'])\n",
    "            #del row['dimensionDiameter'][0]\n",
    "                #print(row['dimensionDiameter'])\n",
    "    #elif math.isnan(row['dimensionDiameter']):\n",
    "        #print('Nonono')\n",
    "\n",
    "    return row\n",
    "def cleanNans(series, listOfFields):\n",
    "    cleanseries = pd.Series()\n",
    "    nanseries = pd.Series('object')\n",
    "    if not series[listOfFields].hasnans:\n",
    "        if not series[listOfFields].isnull().values.any():\n",
    "        #print(series[listOfFields])\n",
    "            cleanseries = series\n",
    "\n",
    "    return cleanseries \n",
    "    #thelist = ['temper', 'temperAmount', 'temperParticles']\n",
    "    #notnull = series.notnull()\n",
    "    #good = series.loc[notnull]\n",
    "    #df=pd.DataFrame(series)\n",
    "    #print(df.columns)\n",
    "    #if pd.Series(thelist).isin(df.columns).all():\n",
    "        #print (good['temper'])\n",
    "\n",
    "\n",
    "    #else:\n",
    "        #print('NOLIST: ', series['temper'] )\n",
    "    #res = subset.groupby(DFresources['temper'].map(tuple))['Count'].sum()\n",
    "    #print(series[listOfFields])\n",
    "    #for column in interestproperties:\n",
    "        #listvalues = [str(i) for i in DFresources[column]]\n",
    "        #print(column, set(listvalues) )    return series\n",
    "def typoStat(df):\n",
    "    notnull = df.notnull()\n",
    "    good = df[notnull]\n",
    "    good = good.astype({'temper': 'string'})\n",
    "    res = good.groupby(['temper', 'temperAmount', 'temperParticles']).size()\n",
    "    print(res)\n",
    "    #return res\n",
    "\n",
    "\n",
    "def fillEmptyProcessor(series):\n",
    "    if 'processor' in series and not type(series['processor'])==list:\n",
    "        if math.isnan(series['processor']):\n",
    "            series['processor']=series['created']['user']\n",
    "            print(series[['type','processor']])\n",
    "    \n",
    "    return series\n",
    "\n",
    "\n",
    "def DOCtoDF(DOC):\n",
    "    DFdocs = pd.DataFrame(DOC)\n",
    "    print(DFdocs.columns)\n",
    "    DFdocs = DFdocs.drop('resource', axis=1)\n",
    "    DFresources = pd.DataFrame([i['resource'] for i in DOC])\n",
    "    for col in DFdocs.columns:\n",
    "        DFresources[str(col)]=DFdocs[str(col)]\n",
    "    docfields = DFdocs.columns\n",
    "\n",
    "    return DFresources, docfields\n",
    "   \n",
    "\n",
    "def DFtoDOC(DFresources, docfields):\n",
    "    DF = DFresources\n",
    "    columns = [i for i in DFresources.columns if not i in docfields]\n",
    "    #print(columns)\n",
    "    DOC = []\n",
    "    for index,row in DF.iterrows():\n",
    "        #print(type(row[columns]))\n",
    "        #dd = defaultdict(list)\n",
    "        #print('Before DROP:')  \n",
    "        cleanrow=row.dropna() \n",
    "        #Ã¤print(cleanrow)\n",
    "        row['resource']= cleanrow.to_dict()\n",
    "        #row['resource'] = {k: row['resource'][k] for k in row['resource'] if not isnan(row['resource'][k])}\n",
    "        #print(row)\n",
    "        #print('After DROP:')\n",
    "        row = row.drop(columns)\n",
    "        #print(row)\n",
    "        dict = row.to_dict()\n",
    "        clean_dict = {k: dict[k] for k in dict.keys() if not isinstance(dict[k], (float)) or not isnan(dict[k])}\n",
    "\n",
    "        DOC.append(clean_dict)\n",
    "    \n",
    "    DOChull={}\n",
    "    DOChull['docs']=DOC\n",
    "    return DOChull\n",
    "def simpleTypoPlot (df, listOfFields):\n",
    "    df = good.replace(np.nan,0)\n",
    "    df = df.astype({'temper': 'string'})\n",
    "    df = df.groupby(listOfFields).size()\n",
    "    df.plot.bar()\n",
    "    plt.show()\n",
    " \n",
    "def plot_bargraph_with_groupings(df, groupby, colourby, title, xlabel, ylabel):\n",
    "    \"\"\"\n",
    "    Plots a dataframe showing the frequency of datapoints grouped by one column and coloured by another.\n",
    "    df : dataframe\n",
    "    groupby: the column to groupby\n",
    "    colourby: the column to color by\n",
    "    title: the graph title\n",
    "    xlabel: the x label,\n",
    "    ylabel: the y label\n",
    "    \"\"\"\n",
    "\n",
    "    import matplotlib.patches as mpatches\n",
    "    df = df.astype({'temper': 'string'})\n",
    "\n",
    "    # Makes a mapping from the unique colourby column items to a random color.\n",
    "    ind_col_map = {x:y for x, y in zip(df[colourby].unique(),\n",
    "                               [plt.cm.Paired(np.arange(len(df[colourby].unique())))][0])}\n",
    "    #print(ind_col_map)\n",
    "\n",
    "\n",
    "    # Find when the indicies of the soon to be bar graphs colors.\n",
    "    unique_comb = df[[groupby, colourby]].drop_duplicates()\n",
    "    name_ind_map = {x:y for x, y in zip(unique_comb[groupby], unique_comb[colourby])}\n",
    "    c = df[groupby].value_counts().index.map(lambda x: ind_col_map[name_ind_map[x]])\n",
    "\n",
    "    # Makes the bargraph.\n",
    "    ax = df[groupby].value_counts().plot(kind='bar',\n",
    "                                         figsize=FIG_SIZE,\n",
    "                                         title=title)\n",
    "    # Makes a legend using the ind_col_map\n",
    "    legend_list = []\n",
    "    for key in ind_col_map.keys():\n",
    "        legend_list.append(mpatches.Patch(color=ind_col_map[key], label=key))\n",
    "\n",
    "    # display the graph.\n",
    "    plt.legend(handles=legend_list)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "\n",
    "\n",
    "\n",
    "def addModifiedEntry(doc):\n",
    "    now = datetime.now()\n",
    "    entry = {}\n",
    "    entry['user'] = 'Script mhaibt'\n",
    "    daytoSec = now.strftime('%Y-%m-%dT%H:%M:%S')\n",
    "    sec = \"{:.3f}\".format(Decimal(now.strftime('.%f')))\n",
    "    entry['date'] = daytoSec + str(sec)[1:] + 'Z'\n",
    "    #print(entry)\n",
    "    if not 'modified' in doc.keys():\n",
    "        doc['modified']=[]\n",
    "    doc['modified'].append(entry)\n",
    "\n",
    "    \n",
    "    #print(doc['modified'])\n",
    "    return doc\n",
    "\n",
    "def deleteImagestorefile(series):\n",
    "    figure_imagestorepath = os.path.join('imagestore', 'exportProject', str(series['_id']) )\n",
    "    if os.path.exists(figure_imagestorepath):\n",
    "      os.remove(figure_imagestorepath)\n",
    "    return series\n",
    "\n",
    "def handleduplicate(df, field):\n",
    "    print(len(df[field]))\n",
    "    groupsize = df.groupby( field ).size()\n",
    "    cumcount = df.groupby( field ).cumcount()\n",
    "    df = df.set_index(field)\n",
    "    df['groupsize']= groupsize\n",
    "    df = df.reset_index()\n",
    "    df['cumcount']= cumcount\n",
    "    pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "    #print(df['cumcount'])\n",
    "    dfnew = pd.DataFrame()\n",
    "    df['cumcount'].fillna(0, inplace=True)\n",
    "    df['groupsize'].fillna(0, inplace=True)\n",
    "    print(df[['cumcount','groupsize']])\n",
    "    for index, row in df.iterrows():\n",
    "        if int(row['cumcount']) == 0 and int(row['groupsize'])== 1:\n",
    "            row[field + '_undup'] = row[field]\n",
    "        else: \n",
    "            row[field + '_undup'] = str(row[field]) + '_' + str(row['cumcount'])\n",
    "        dfnew = dfnew.append(row)\n",
    "    return dfnew\n",
    "\n",
    "def pathToStore(series):\n",
    "    series['figure_imagestorepath'] = os.path.join(series['imagestore'], series['exportProject'], str(series['figure_tmpid']) )\n",
    "    return series\n",
    "def imageToStore(series):\n",
    "    shutil.copyfile(str(series['figure_path']), series['figure_imagestorepath'])\n",
    "    return series\n",
    "def copytree(src, dst, symlinks=False, ignore=None):\n",
    "    for item in os.listdir(src):\n",
    "        s = os.path.join(src, item)\n",
    "        d = os.path.join(dst, item)\n",
    "        if os.path.isdir(s):\n",
    "            shutil.copytree(s, d, symlinks, ignore)\n",
    "        else:\n",
    "            shutil.copy2(s, d)\n",
    "\n",
    "def findDocstoUpdateandCreate(targetdbrows, sourcedbrows):\n",
    "    newdocs_create = []\n",
    "    newdocs_update = []\n",
    "  \n",
    "     \n",
    "    \n",
    "\n",
    "    for newrow in sourcedbrows['rows']:\n",
    "        oldrow = next((oldrow for oldrow in targetdbrows['rows'] if oldrow['id'] == newrow['id']), None)\n",
    "        if not oldrow:\n",
    "            print('This row doesnt exist in targetdb: ', oldrow)\n",
    "            newdocs_create.append(newrow)\n",
    "        if oldrow:\n",
    "            print('This row exists in targetdb: ', oldrow)\n",
    "            newdocs_update.append(newrow)\n",
    "    return newdocs_create, newdocs_update \n",
    "\n",
    "def newdocsUpdate(db, targetdb, newdocs_update):\n",
    "    sourcedocs_toupdate = getDocsInList(db_name=db, list=[newdoc['id'] for newdoc in newdocs_update])\n",
    "    sourcedocs_toupdate_rev = []\n",
    "    for sourcedoc in sourcedocs_toupdate['docs']:\n",
    "        match = next((newrow for newrow in newdocs_update if newrow['id'] == sourcedoc['_id']), None)\n",
    "        sourcedoc['_rev']=match['value']['rev']\n",
    "        remove_key = sourcedoc.pop('_attachments', None) \t\n",
    "        sourcedocs_toupdate_rev.append(sourcedoc)\n",
    "        if 'resource' in sourcedoc.keys() and 'type' in sourcedoc['resource'] and sourcedoc['resource']['type'] == 'Drawing':\n",
    "            shutil.copyfile(os.path.join(imagestore, db, sourcedoc['_id']), os.path.join(imagestore, targetdb, sourcedoc['_id']))\n",
    "    docshull = {}\n",
    "    docshull['docs']=sourcedocs_toupdate_rev\n",
    "    #print(json.dumps(docshull['docs'], indent=4, sort_keys=True))\n",
    "    pouchDB_url_bulk = f'{db_url}/{targetdb}/_bulk_docs'\n",
    "    bulkSaveChanges(docshull, pouchDB_url_bulk, auth)\n",
    "\n",
    "\n",
    "\n",
    "def newdocsCreate(db, targetdb, newdocs_create):\n",
    "    sourcedocs_tocreate = getDocsInList(db_name=db, list=[newdoc['id'] for newdoc in newdocs_create])\n",
    "    sourcedocs_tocreate = [doc for doc in sourcedocs_tocreate['docs'] if 'resource' in doc.keys() and not doc['resource']['id'] == 'project']\n",
    "    sourcedocs_tocreate_prepare = []\n",
    "    for doc in sourcedocs_tocreate:\n",
    "        remove_key = doc.pop('_attachments', None) \t\n",
    "        remove_key = doc.pop('_rev', None)\n",
    "        #remove_key = doc['resource'].pop('featureVectors', None)\n",
    "        if doc['resource']['type'] == 'Drawing':\n",
    "            sourceimagefile = os.path.join(imagestore, db, doc['_id'])\n",
    "            targetimagefile = os.path.join(imagestore, targetdb, doc['_id'])\n",
    "            shutil.copyfile(sourceimagefile, targetimagefile)\n",
    "        sourcedocs_tocreate_prepare.append(doc)    \n",
    "    docshull = {}\n",
    "    docshull['docs']=sourcedocs_tocreate_prepare\n",
    "\n",
    "    pouchDB_url_bulk = f'{db_url}/{targetdb}/_bulk_docs'\n",
    "    bulkSaveChanges(docshull, pouchDB_url_bulk, auth)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "### Setup connection to Field-database-server based on config.json file ###\n",
    "config = pdfield.loadconfigs('config\\config.json')\n",
    "api = pdfield.couchDB_APIs(config, db_name='uruk')\n",
    "api_catalogs = pdfield.couchDB_APIs(config, db_name='urukcatalogs_ed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import all documents from Target Field-database ###\n",
    "targetDOCs= pdfield.getAllDocs(api)\n",
    "targetDF, docfields  = pdfield.allDocsToDf(targetDOCs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for db in selectlist:\n",
    "    sourcedbrows = getAllDocsv2(db_name = db)\n",
    "    allsourcedbrows = allsourcedbrows + sourcedbrows['rows']\n",
    "    newdocs_create, newdocs_update = findDocstoUpdateandCreate(targetdbrows, sourcedbrows)\n",
    "\n",
    "    newdocsUpdate(db, targetdb, newdocs_update)\n",
    "    newdocsCreate(db, targetdb, newdocs_create)\n",
    "\n",
    "   \n",
    "olddocs_delete = []\n",
    "\n",
    "for oldrow in targetdbrows['rows']:\n",
    "        newrow = next((newrow for newrow in allsourcedbrows if newrow['id'] == oldrow['id']), None)\n",
    "        if not newrow:\n",
    "            print('This row doesnt exist in sourcedb: ', oldrow)\n",
    "            olddocs_delete.append(oldrow)\n",
    "\n",
    "\n",
    "\n",
    "def olddocsDelete(olddocs_delete, targetdb):\n",
    "    olddocs_delete_preparelist = []\n",
    "    for doc in olddocs_delete:\n",
    "        olddocs_delete_prepare = {}\n",
    "        olddocs_delete_prepare['_id'] = doc['id']\n",
    "        olddocs_delete_prepare['_rev'] = doc['value']['rev']\n",
    "        olddocs_delete_prepare['_deleted'] = True\n",
    "        olddocs_delete_preparelist.append(olddocs_delete_prepare)\n",
    "    docshull = {}\n",
    "    docshull['docs']= olddocs_delete_preparelist\n",
    "    #print(json.dumps(docshull, indent=4, sort_keys=True))\n",
    "    pouchDB_url_bulk = f'{db_url}/{targetdb}/_bulk_docs'\n",
    "    bulkSaveChanges(docshull, pouchDB_url_bulk, auth)\n",
    "print ('So many will be deleted intargetdb: ', len(olddocs_delete))\n",
    "olddocsDelete(olddocs_delete, targetdb)\n",
    "\n",
    "def deleteDrawingsImagestore(targetdb):\n",
    "    drawings = getAllDrawings(targetdb)\n",
    "    #print(drawings)\n",
    "    targetimagestore = os.path.join(imagestore, targetdb)\n",
    "    for image in os.listdir(targetimagestore):\n",
    "        drawing = next((drawing for drawing in drawings['docs'] if drawing['_id'] == str(image)), None)\n",
    "        if not drawing:\n",
    "            print('This image is not in targetdb drawings: ', image)\n",
    "            os.remove(os.path.join(targetimagestore, image))\n",
    "\n",
    "\n",
    "\n",
    "deleteDrawingsImagestore(targetdb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('accessidaifield')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "10c33de7b628476e462a2e3318cbd1f752b8da65d7799286a80a558fcb49b004"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
